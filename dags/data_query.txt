import airflow
from datetime import timedelta
from airflow.models import DAG
from airflow.operators.bash_operator import BashOperator
from airflow.operators.dummy_operator import DummyOperator

args = {
    "owner": "gizelly",
    "description": "spark Consumer via bash Operator in same container",
    "start_date": airflow.utils.dates.days_ago(1),
    "provide_context": True,
}

dag = DAG(
    dag_id="3_data_query_and_save",
    default_args=args,
    schedule_interval="@once",
    catchup=False,
)

task0 = DummyOperator(task_id="start", dag=dag)

task1 = BashOperator(
    task_id="local_queries",
    bash_command="/opt/spark-2.3.1-bin-hadoop2.7/bin/spark-submit "
    "--master local[*] "
    "/usr/local/airflow/dags/src/spark_consume_data/local_queries.py",
    dag=dag,
)

task2 = BashOperator(
    task_id="ride_amount_queries",
    bash_command="/opt/spark-2.3.1-bin-hadoop2.7/bin/spark-submit "
    "--master local[*] "
    "/usr/local/airflow/dags/src/spark_consume_data/ride_amount_queries.py",
    dag=dag,
)

task0 >> [task1, task2]
